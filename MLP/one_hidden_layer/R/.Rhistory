dt <- fread("~/Dropbox/--- Deep Learning/# Lecture slides/# Data/Data_normalized.csv")
library(data.table)
dt <- fread("~/Dropbox/--- Deep Learning/# Lecture slides/# Data/Data_normalized.csv")
dt
dt <- fread("~/Dropbox/--- Deep Learning/# Lecture slides/# Data/Data_normalized.csv")
dt[, V1 := NULL]
dt
y <- as.matrix(dt[, y])
y <- as.matrix(dt[, Y])
y
X <- as.matrix(dt[, list(X1, X2)])
X
w1 <- matrix(data = c(-0.2, -0.4, 0.4, -0.7, -1.0, -0.8), nrow=2)
w1 <- matrix(data = c(-0.2, -0.4, 0.4, -0.7, -1.0, -0.8), nrow=2)
w1
w2 <- matrix(data = c(-0.6, -0.6, -0.3), nrow=1)
w2
b1 <- 0.1
b2 <- -0.2
b3 <- 0.4
b_HL <- matrix(data = c(b1, b2, b3), nrow=1)
B_HL <- rbind(b_HL, b_HL, b_HL, b_HL, b_HL)
bon <- -0.6
B_HL
matrix(b_HL, nrow = 10, ncol = 3)
b_HL
t(matrix(b_HL, nrow = 3, ncol = 10))
B_HL
bon <- -0.6  # Bias of ON
ls()
rm(list=ls())
rm(list=ls())
dt <- fread("~/Dropbox/--- Deep Learning/# Lecture slides/# Data/Data_normalized.csv")
dt[, V1 := NULL]
dt
y <- as.matrix(dt[, Y])
X <- as.matrix(dt[, list(X1, X2)])
w1 <- matrix(data=c(-0.2, -0.4, 0.4, -0.7, -1.0, -0.8), nrow=2)
w1
w2 <- matrix(data=c(-0.6, -0.6, -0.3), nrow=1)
w2
b1 <- 0.1  # Bias of H1
b2 <- -0.2  # Bias of H2
b3 <- 0.4  # Bias of H3
b_HL <- matrix(data=c(b1, b2, b3), nrow=1)  # Matrix of bias
B_HL <- t(matrix(b_HL, nrow=3, ncol=10)) # Stacked matix of bias, dim = N
bon <- -0.6  # Bias of ON
f_sigmoid <- function(x) 1 / (1 + exp(-x))
A1 <- X %*% w1 + B_HL  # Sum of inputs
X %*% w1
Z1 <- f_sigmoid(A1)  # Activated
Z1
round(Z1, 2)
A2 <- Z1 %*% t(w2) + bon  # Sum of inputs
A2
y_hat <- f_sigmoid(A2)  # Activated
y_hat
round(cost, 2)  # For slides
cost <- f_cost(y, y_hat)  # Cost of each observation
cost
round(cost, 2)  # For slides
f_cost <- function(y, yhat) 0.5 * (y - yhat)^2  # Quadratic cost function
A1 <- X %*% w1 + B_HL  # Sum of inputs
A1
Z1 <- f_sigmoid(A1)  # Activated
Z1
round(Z1, 2)  # For slides
A2 <- Z1 %*% t(w2) + bon  # Sum of inputs
A2
y_hat <- f_sigmoid(A2)  # Activated
y_hat
round(y_hat, 2)  # For slides
cost <- f_cost(y, y_hat)  # Cost of each observation
cost
round(cost, 2)  # For slides
sum(cost)  # Cost of model
alpha <- 0.5
error <- -(y - y_hat)
sum(cost)  # Cost of model
rnorm(c(3,4), 0, 2)
w1 = matrix(rnorm((3*4), 0, 2), nrow=3, ncol=4)
w1
w2 = matrix(rnorm((4*1), 0, 2), nrow=3, ncol=4)
w2 = matrix(rnorm((4*1), 0, 2), nrow=4)
w2
w2 = matrix(rnorm(4, 0, 2), nrow=4)
w2
Z1 <- 1 / (1 + exp(-(X %*% w1 + B_HL)))
X
dt <- fread("~/Dropbox/--- Deep Learning/# Lecture slides/# Data/Data_normalized.csv")
dt[, V1 := NULL]
dt
y <- as.matrix(dt[, Y])
X <- as.matrix(dt[, list(X1, X2)])
w1 <- matrix(data=c(-0.2, -0.4, 0.4, -0.7, -1.0, -0.8), nrow=2)
w1
w2 <- matrix(data=c(-0.6, -0.6, -0.3), nrow=1)
w2
Z1 <- 1 / (1 + exp(-(X %*% w1 + B_HL)))
Z1
y_hat <- 1 / (1 + exp(-(Z1 %*% t(w2) + bon)))
y_hat
ON_error <- (y - y_hat)
ON_error
y_hat
(1 - y_hat)
(y_hat * (1 - y_hat))
w2
ON_error
ON_error * (y_hat * (1 - y_hat))
Z1
bpe_ON
bpe_ON <- ON_error * (y_hat * (1 - y_hat))
bpe_ON
library(data.table)
rm(list=ls())
dt <- fread("~/Dropbox/--- Deep Learning/# Lecture slides/# Data/Data_normalized.csv")
dt[, V1 := NULL]
dt
y <- as.matrix(dt[, Y])
X <- as.matrix(dt[, list(X1, X2)])
w1 <- matrix(data=c(-0.2, -0.4, 0.4, -0.7, -1.0, -0.8), nrow=2)
w1
w2 <- matrix(data=c(-0.6, -0.6, -0.3), nrow=1)
w2
b1 <- 0.1  # Bias of H1
b2 <- -0.2  # Bias of H2
b3 <- 0.4  # Bias of H3
b_HL <- matrix(data=c(b1, b2, b3), nrow=1)  # Matrix of bias
B_HL <- t(matrix(b_HL, nrow=3, ncol=10)) # Stacked matix of bias, dim = N
bon <- -0.6  # Bias of ON
f_sigmoid <- function(x) 1 / (1 + exp(-x))  # Non-linear activation function
f_sigmoid_deriv <- function(x) f_sigmoid(x) / (1 - f_sigmoid(x))  # Derivative
f_cost <- function(y, yhat) 0.5 * (y - yhat)^2  # Quadratic cost function
alpha <- 0.5  # Learning rate
error <- -(y - y_hat)
Z1 <- 1 / (1 + exp(-(X %*% w1 + B_HL)))
Z1
y_hat <- 1 / (1 + exp(-(Z1 %*% t(w2) + bon)))
y_hat
Z1
X %*% w1
X %*% w1 + B_HL
exp(-(X %*% w1 + B_HL))
1 / (1 + exp(-(X %*% w1 + B_HL)))
X %*% w1 + B_HL
-(X %*% w1 + B_HL)
exp(0.6148140)
exp(-0.6148140)
1/(1+exp(-0.6148140))
1/(1+exp(0.6148140))
Z1
round(z1, 2)
round(Z1, 2)
X %*% w1 + B_HL
y_hat <- 1 / (1 + exp(-(Z1 %*% t(w2) + bon)))
round(y_hat, 2)
Z1 %*% t(w2) + bon
A1 <- X %*% w1 + B_HL  # Sum of inputs
A1
Z1 <- f_sigmoid(A1)  # Activated
Z1
round(Z1, 1)
A2 <- Z1 %*% t(w2) + bon  # Sum of inputs
A2
y_hat <- f_sigmoid(A2)  # Activated
y_hat
round(y_hat, 2)  # For slides
cost <- f_cost(y, y_hat)  # Cost of each observation
cost
round(cost, 2)  # For slides
sum(cost)  # Cost of model
A1
Z1
round(Z1, 1)  # For slides
A2
round(y_hat, 2)  # For slides
round(cost, 2)  # For slides
error <- -(y - y_hat)
error
A2
error * f_sigmoid_deriv(A2)
f_sigmoid_deriv(A2)
A2
error * f_sigmoid_deriv(A2)
bpe_ON <- error * f_sigmoid_deriv(A2)
round(bpe_ON, 2)
Z1 <- 1 / (1 + exp(-(X %*% w1 + B_HL)))
y_hat <- 1 / (1 + exp(-(Z1 %*% t(w2) + bon)))
ON_error <- (y - y_hat)
bpe_ON <- ON_error * (y_hat * (1 - y_hat))
w2 <- w2 + alpha * t(Z1) %*% bpe_ON  # Update w2
HL_error <- bpe_ON %*% w2
bpe_HL <- HL_error * (Z1 * (1 - Z1))
w1 <- w1 + alpha * X %*% (bpe_HL)  # Update w1
alpha
Z1
bpe_ON
f_sigmoid_deriv <- function(x) f_sigmoid(x) * (1 - f_sigmoid(x))  # Derivative
error <- -(y - y_hat)
bpe_ON <- error * f_sigmoid_deriv(A2)
bpe_ON
round(bpe_ON, 2)
round(bpe_ON, 3)
A2
f_sigmoid_deriv
f_sigmoid_deriv(A2)
error * f_sigmoid_deriv(A2)
ON_error
Z1 <- 1 / (1 + exp(-(X %*% w1 + B_HL)))
y_hat <- 1 / (1 + exp(-(Z1 %*% t(w2) + bon)))
ON_error <- (y - y_hat)
ON_error
error
ON_error <- -(y - y_hat)
ON_error
(y_hat * (1 - y_hat))
f_sigmoid_deriv(A2)
bpe_ON <- ON_error * (y_hat * (1 - y_hat))
bpe_ON <- ON_error * (y_hat * (1 - y_hat))
bpe_ON
t(Z1)
t(Z1) %*% bpe_ON
alpha * t(Z1) %*% bpe_ON
w2
w2 <- matrix(data=c(-0.6, -0.6, -0.3), nrow=3)
w2
A2 <- Z1 %*% w2 + bon  # Sum of inputs
A2
w2 <- w2 + alpha * t(Z1) %*% bpe_ON  # Update w2
w2
bpe_ON
w2
w2 <- matrix(data=c(-0.6, -0.6, -0.3), nrow=3)
w2
bpe_ON
bpe_ON %*% w2
w2 + alpha * t(Z1) %*% bpe_ON
Z1
Z1[1,]
w2 + alpha * t(Z1) %*% bpe_ON
bpe_ON[1]
w2 + alpha * t(Z1[1,]) %*% bpe_ON[1]
t(Z1[1,]) %*% bpe_ON[1]
t(Z1[1,]) * bpe_ON[1]
w2 + alpha * t(Z1[1,]) * bpe_ON[1]
Z1[1,] * bpe_ON[1]
w2
t(Z1[1,]) * bpe_ON[1]
t(Z1[1,] * bpe_ON[1])
t(t(Z1[1,] * bpe_ON[1]))
w2 + alpha * t(t(Z1[1,] * bpe_ON[1]))
bon + alpha * t(Z1)
bon + alpha * bpe_ON[1]
bpe_ON[1]
bon
bpe_ON
w2
bpe_ON %*% w2
bpe_ON[1,] * w2[1]
Z1
(1 - Z1)
bpe_ON[1,] * w2[1] * (Z1[1,1] * (1 - Z1[1,1]))
Z1[1,1]
Z1
A1
bpe_ON[1,] * w2[1]
bpe_ON[1,] * w2[1] * (Z1[1,1] * (1 - Z1[1,1]))
bpe_ON
w2
w1
(Z1 * (1 - Z1))
(Z1 * (1 - Z1)) == f_sigmoid(A1) * (1 - f_sigmoid(A1))
(Z1 * (1 - Z1)) == mapply(A1, FUN = function(x) f_sigmoid(x) * (1-f_sigmoid(x)))
w2
(Z1 * (1 - Z1))
f_1_Z1 <- (Z1 * (1 - Z1))
t(w2) * f_1_Z1
w2
matrix(w2, nrow= 10, ncol = 3)
t(matrix(w2, nrow= 3, ncol = 10))
w2
t(matrix(w2, nrow= 3, ncol = 10)) * f_1_Z1
(Z1[1,1] * (1 - Z1[1,1])) + w2[1]
(Z1[1,1] * (1 - Z1[1,1])) * w2[1]
(Z1[1,2] * (1 - Z1[1,2])) * w2[2]
bpe_ON
t(matrix(w2, nrow=3, ncol=10))
matrix(bpe_ON, nrow=10, ncol=3)
matrix(bpe_ON, nrow=10, ncol=3) * t(matrix(w2, nrow=3, ncol=10))
library(data.table)
rm(list=ls())
dt[, V1 := NULL]
dt <- fread("~/Dropbox/--- Deep Learning/# Lecture slides/# Data/Data_normalized.csv")
dt
y <- as.matrix(dt[, Y])
X <- as.matrix(dt[, list(X1, X2)])
w1 <- matrix(data=c(-0.2, -0.4, 0.4, -0.7, -1.0, -0.8), nrow=2)
w1
w2 <- matrix(data=c(-0.6, -0.6, -0.3), nrow=3)
w2
b1 <- 0.1  # Bias of H1
b2 <- -0.2  # Bias of H2
b3 <- 0.4  # Bias of H3
b_HL <- matrix(data=c(b1, b2, b3), nrow=1)  # Matrix of bias
B_HL <- t(matrix(b_HL, nrow=3, ncol=10)) # Stacked matix of bias, dim = N
bon <- -0.6  # Bias of ON
f_sigmoid <- function(x) 1 / (1 + exp(-x))  # Non-linear activation function
f_sigmoid_deriv <- function(x) f_sigmoid(x) * (1 - f_sigmoid(x))  # Derivative
f_cost <- function(y, yhat) 0.5 * (y - yhat)^2  # Quadratic cost function
f_cost <- function(y, yhat) 0.5 * sum((y - yhat)^2)  # Quadratic cost function
cost <- f_cost(y, y_hat)  # Cost of each observation
A1 <- X %*% w1 + B_HL  # Sum of inputs
A1
Z1 <- f_sigmoid(A1)  # Activated
Z1
round(Z1, 1)  # For slides
A2 <- Z1 %*% w2 + bon  # Sum of inputs
A2
y_hat <- f_sigmoid(A2)  # Activated
y_hat
round(y_hat, 2)  # For slides
cost <- f_cost(y, y_hat)  # Cost of each observation
cost
round(cost, 2)  # For slides
alpha <- 0.5  # Learning rate
error <- -(y - y_hat)
bpe_ON <- error * f_sigmoid_deriv(A2)
BPE_ON <- cbind(bpe_ON, bpe_ON, bpe_ON)
w2_new <- w2 - alpha * BPE_ON[1,] * Z1[1,]
bon_new <- bon - alpha * BPE_ON
w2_new
bpe_ON <- error * f_sigmoid_deriv(A2)
bpe_ON
BPE_ON <- cbind(bpe_ON, bpe_ON, bpe_ON)
Z1 <- 1 / (1 + exp(-(X %*% w1 + B_HL)))
Z1
y_hat <- 1 / (1 + exp(-(Z1 %*% w2 + bon)))
y_hat
ON_error <- -(y - y_hat)
ON_error
bpe_ON <- ON_error * (y_hat * (1 - y_hat))
bpe_ON
Z1
bpe_ON
alpha * t(Z1) %*% bpe_ON
t(Z1)
t(Z1) %*% bpe_ON
t(Z1)
t(Z1)[1,]
t(Z1)[1,] %*% bpe_ON
X
matrix(bpe_ON, nrow=10, ncol=3)
X
X$X1
X[,1]
bpe_HL
HL_error <- matrix(bpe_ON, nrow=10, ncol=3) * t(matrix(w2, nrow=3, ncol=10))
bpe_HL <- HL_error * (Z1 * (1 - Z1))
bpe_HL
X[,1] %*% bpe_HL
rbind(X[,1] %*% bpe_HL, X[,2] %*% bpe_HL)
w1
w1 - alpha * rbind(X[,1] %*% bpe_HL, X[,2] %*% bpe_HL)  # Update w1
bpe_ON
matrix(1, nrow=10, ncol=3)
bpe_ON * matrix(1, nrow=10, ncol=3)
X
Y
y
y_hat
X
round(X, 2)
X <- round(X, 2)
w1 <- matrix(data=c(-0.2, -0.4, 0.4, -0.7, -1.0, -0.8), nrow=2)
w1
w2 <- matrix(data=c(-0.6, -0.6, -0.3), nrow=3)
w2
b1 <- 0.1  # Bias of H1
b2 <- -0.2  # Bias of H2
b3 <- 0.4  # Bias of H3
b_HL <- matrix(data=c(b1, b2, b3), nrow=1)  # Matrix of bias
B_HL <- t(matrix(b_HL, nrow=3, ncol=10)) # Stacked matix of bias, dim = N
bon <- -0.6  # Bias of ON
f_sigmoid <- function(x) 1 / (1 + exp(-x))  # Non-linear activation function
f_sigmoid_deriv <- function(x) f_sigmoid(x) * (1 - f_sigmoid(x))  # Derivative
f_cost <- function(y, yhat) 0.5 * sum((y - yhat)^2)  # Quadratic cost function
A1 <- X %*% w1 + B_HL  # Sum of inputs
A1
Z1 <- f_sigmoid(A1)  # Activated
Z1
round(Z1, 1)  # For slides
A2 <- Z1 %*% w2 + bon  # Sum of inputs
A2
y_hat <- f_sigmoid(A2)  # Activated
y_hat
-(y - y_hat)
f_sigmoid_deriv(A2)
error <- -(y - y_hat)
bpe_ON <- error * f_sigmoid_deriv(A2)
bpe_ON
matrix(bpe_ON, nrow=10, ncol=3) * t(matrix(w2, nrow=3, ncol=10))
HL_error * (Z1 * (1 - Z1))
